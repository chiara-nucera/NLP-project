{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7c17eb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CWD: C:\\Users\\chiar\\Desktop\\NLP Project\\0. File potenzialmente finale - Copy\n",
      "SRC in sys.path: True\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path #Imports Path for filesystem operations\n",
    "import os, sys #Imports OS and system utilities\n",
    "\n",
    "PROJECT_ROOT = Path.cwd().resolve() #Starts from the current working directory\n",
    "for _ in range(3): #Tries to find the project root by moving up the directory tree\n",
    "    if (PROJECT_ROOT / \"src\").exists() and (PROJECT_ROOT / \"configs\").exists(): #Checks for project structure\n",
    "        break #Stops if project root is found\n",
    "    PROJECT_ROOT = PROJECT_ROOT.parent #Moves one directory up\n",
    "\n",
    "os.chdir(PROJECT_ROOT) #Changes the working directory to the project root\n",
    "\n",
    "SRC = PROJECT_ROOT / \"src\" #Defines the src directory path\n",
    "if str(SRC) not in sys.path:\n",
    "    sys.path.insert(0, str(SRC)) #Adds src to Python path if missing\n",
    "\n",
    "print(\"CWD:\", Path.cwd()) #Prints current working directory\n",
    "print(\"SRC in sys.path:\", str(SRC) in sys.path) #Checks if src is in Python path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "567758b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragtrust.config import load_config\n",
    "cfg = load_config(\"configs/with_training.yaml\")  # o zero_training.yaml\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf0a81e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\chiar\\envtwo\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\chiar\\envtwo\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "CORE mode: generation disabled -> False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 51/51 [00:36<00:00,  1.41it/s]\n",
      "Some weights of the model checkpoint at roberta-large-mnli were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Poison enabled: False rate: 0.0\n",
      "Retrieved: 20 Poisoned list: 20 Injected poison: 0\n",
      "FEVER BASE summary: {'acc': 0.25, 'hallucination': 0.4342105263157895}\n",
      "\n",
      "=== CORE RUN | poison enabled: False rate: 0.0 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 51/51 [00:36<00:00,  1.41it/s]\n",
      "Some weights of the model checkpoint at roberta-large-mnli were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FEVER summary: {'acc': 0.25, 'hallucination': 0.4342105263157895}\n",
      "\n",
      "=== CORE RUN | poison enabled: True rate: 0.1 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 51/51 [00:49<00:00,  1.03it/s]\n",
      "Some weights of the model checkpoint at roberta-large-mnli were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FEVER summary: {'acc': 0.23684210526315788, 'hallucination': 0.40789473684210525}\n",
      "\n",
      "=== CORE RUN | poison enabled: True rate: 0.2 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 51/51 [00:26<00:00,  1.94it/s]\n",
      "Some weights of the model checkpoint at roberta-large-mnli were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FEVER summary: {'acc': 0.23684210526315788, 'hallucination': 0.40789473684210525}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 7/7 [00:00<00:00, 29.92it/s]\n",
      "Some weights of the model checkpoint at roberta-large-mnli were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HotpotQA summary: {'acc': 0.0, 'hallucination': 0.005}\n"
     ]
    }
   ],
   "source": [
    "from ragtrust.utils import set_seed #Imports utility to fix random seeds\n",
    "from ragtrust.data.fever import load_fever_examples, extract_gold_titles, build_fever_corpus #Imports FEVER data helpers\n",
    "from ragtrust.data.hotpotqa import load_hotpot_examples, build_hotpot_corpus #Imports HotpotQA data helpers\n",
    "from ragtrust.experiment import RAGTrustPipeline #Imports the main RAG pipeline\n",
    "from ragtrust.evaluation.run_experiment import run_fever, run_hotpot #Imports evaluation runners\n",
    "from dataclasses import replace #Imports replace to safely modify configs\n",
    "\n",
    "#CORE RUN (generation: disabled)\n",
    "cfg = replace(cfg, generation=replace(cfg.generation, enabled=False, num_samples=1)) #Disables generation for core evaluation\n",
    "print(\"CORE mode: generation disabled ->\", cfg.generation.enabled) #Prints generation status\n",
    "#Fine New9\n",
    "\n",
    "set_seed(cfg.seed) #Fixes random seed for reproducibility\n",
    "\n",
    "results = {} #Initializes dictionary to store results\n",
    "\n",
    "#FEVER\n",
    "if cfg.data.fever.enabled: #Checks if FEVER is enabled\n",
    "    fever_ex = load_fever_examples(cfg.data.fever.train_jsonl, cfg.data.fever.max_examples) #Loads FEVER examples\n",
    "    fever_ex = [ex for ex in fever_ex if ex.label in (\"SUPPORTS\", \"REFUTES\")] #Keeps only labeled examples\n",
    "\n",
    "    wanted = set() #Initializes set of required Wikipedia titles\n",
    "    for ex in fever_ex:\n",
    "        wanted |= extract_gold_titles(ex.evidence) #Collects gold evidence titles\n",
    "\n",
    "    corpus_fever = build_fever_corpus( #Builds FEVER corpus\n",
    "        cfg.data.fever.wiki_pages_dir, #Wikipedia pages directory\n",
    "        wanted_titles=wanted,  #Speeds up corpus construction by filtering pages\n",
    "        max_sentences=cfg.retrieval.max_corpus_sentences, #Limits corpus size\n",
    "    )\n",
    "\n",
    "    rates = [0.0, 0.1, 0.2]   #Defines poisoning rates (0.0 is clean baseline)\n",
    "\n",
    "    # --- BASELINE (no poison): cfg + pipeline coerenti ---\n",
    "    cfg_base = replace(cfg, poisoning=replace(cfg.poisoning, enabled=False, rate=0.0)) #Creates clean baseline config\n",
    "    pipeline_fever_base = RAGTrustPipeline(cfg_base, corpus_fever) #Builds baseline FEVER pipeline\n",
    "\n",
    "    # Sanity check poisoning (baseline)\n",
    "    ex0 = fever_ex[0] #Selects one example for sanity check\n",
    "    out0 = pipeline_fever_base.run_one(ex0.claim, verifier_mode=\"single\") #Runs pipeline on one claim\n",
    "\n",
    "    n_total = len(out0[\"poisoned\"]) #Counts total retrieved passages\n",
    "    n_poison = sum(1 for p in out0[\"poisoned\"] if p.get(\"is_poison\") == \"1\") #Counts poisoned passages\n",
    "\n",
    "    print(\"Poison enabled:\", cfg_base.poisoning.enabled, \"rate:\", cfg_base.poisoning.rate) #Prints poisoning status\n",
    "    print(\"Retrieved:\", len(out0[\"retrieved\"]), \"Poisoned list:\", n_total, \"Injected poison:\", n_poison) #Prints sanity check info\n",
    "\n",
    "    fever_res_base = run_fever(cfg_base, pipeline_fever_base, fever_ex) #Runs FEVER baseline evaluation\n",
    "    results[\"fever_base\"] = fever_res_base #Stores baseline results\n",
    "    print(\"FEVER BASE summary:\", fever_res_base[\"summary\"]) #Prints baseline summary\n",
    "\n",
    "    #CORE RUN: clean vs poisoned (each run uses its own coherent cfg + pipeline)\n",
    "    for r in rates: #Loops over poisoning rates\n",
    "        cfg_r = replace(cfg, poisoning=replace(cfg.poisoning, enabled=(r > 0.0), rate=r)) #Creates config for this rate\n",
    "        print(\"\\n=== CORE RUN | poison enabled:\", cfg_r.poisoning.enabled, \"rate:\", cfg_r.poisoning.rate, \"===\") #Prints run info\n",
    "\n",
    "        pipeline_r = RAGTrustPipeline(cfg_r, corpus_fever) #Builds pipeline for this run\n",
    "        res_r = run_fever(cfg_r, pipeline_r, fever_ex) #Runs FEVER evaluation\n",
    "\n",
    "        results[f\"fever_poison_{r}\"] = res_r #Stores results for this rate\n",
    "        print(\"FEVER summary:\", res_r[\"summary\"]) #Prints summary\n",
    "\n",
    "#HotpotQA\n",
    "if cfg.data.hotpotqa.enabled: #Checks if HotpotQA is enabled\n",
    "    hotpot_ex = load_hotpot_examples(cfg.data.hotpotqa.train_json, cfg.data.hotpotqa.max_examples) #Loads HotpotQA examples\n",
    "\n",
    "    corpus_hotpot = build_hotpot_corpus(hotpot_ex) #Builds HotpotQA corpus\n",
    "    pipeline_hotpot = RAGTrustPipeline(cfg, corpus_hotpot) #Builds HotpotQA pipeline\n",
    "\n",
    "    hotpot_res = run_hotpot(cfg, pipeline_hotpot, hotpot_ex) #Runs HotpotQA evaluation\n",
    "    results[\"hotpotqa\"] = hotpot_res #Stores HotpotQA results\n",
    "    print(\"HotpotQA summary:\", hotpot_res[\"summary\"]) #Prints HotpotQA summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a70bd220",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ABLATION RULE: strict ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 51/51 [00:30<00:00,  1.67it/s]\n",
      "Some weights of the model checkpoint at roberta-large-mnli were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary: {'acc': 0.2565789473684211, 'hallucination': 0.5263157894736842}\n",
      "\n",
      "=== ABLATION RULE: balanced ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 51/51 [00:27<00:00,  1.83it/s]\n",
      "Some weights of the model checkpoint at roberta-large-mnli were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary: {'acc': 0.26973684210526316, 'hallucination': 0.5789473684210527}\n",
      "\n",
      "=== ABLATION RULE: conservative ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 51/51 [00:29<00:00,  1.75it/s]\n",
      "Some weights of the model checkpoint at roberta-large-mnli were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary: {'acc': 0.2565789473684211, 'hallucination': 0.4934210526315789}\n"
     ]
    }
   ],
   "source": [
    "#Running a decision rule ablation experiment\n",
    "from ragtrust.ablation.ablation import run_decision_rule_ablation, DECISION_RULES #Imports ablation function and rules\n",
    "\n",
    "ablation_results = run_decision_rule_ablation( #Runs the decision rule ablation\n",
    "    cfg=cfg_base, #Uses the clean baseline configuration\n",
    "    corpus=corpus_fever, #Uses the FEVER corpus\n",
    "    examples=fever_ex, #Uses FEVER evaluation examples\n",
    "    run_fn=run_fever, #Uses the FEVER evaluation function\n",
    "    rules=DECISION_RULES, #Applies all predefined decision rules\n",
    "    n=200 #Limits the number of examples for the ablation\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b62c5b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e721f75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GEN mode: generation enabled -> True num_samples -> 10\n"
     ]
    }
   ],
   "source": [
    "#Enabling answer generation and self-consistency evaluation.\n",
    "cfg_gen = replace( #Creates a modified configuration for generation mode\n",
    "    cfg,\n",
    "    generation=replace(cfg.generation, enabled=True, num_samples=10), #Enables generation and sets number of samples\n",
    "    evaluation=replace(cfg.evaluation, compute_self_consistency=True), #Enables self-consistency evaluation\n",
    ")\n",
    "print(\"GEN mode: generation enabled ->\", cfg_gen.generation.enabled, \"num_samples ->\", cfg_gen.generation.num_samples) #Prints generation status\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75d8052f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== GEN RUN | poison enabled: False rate: 0.0 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 51/51 [00:33<00:00,  1.54it/s]\n",
      "Some weights of the model checkpoint at roberta-large-mnli were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FEVER summary: {'acc': 0.25, 'hallucination': 0.4342105263157895, 'self_consistency': 0.3085526315789474}\n",
      "\n",
      "=== GEN RUN | poison enabled: True rate: 0.2 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 51/51 [00:32<00:00,  1.59it/s]\n",
      "Some weights of the model checkpoint at roberta-large-mnli were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FEVER summary: {'acc': 0.23684210526315788, 'hallucination': 0.40789473684210525, 'self_consistency': 0.3414473684210526}\n"
     ]
    }
   ],
   "source": [
    "#Running generation-based experiments on FEVER.\n",
    "# It compares clean vs poisoned settings while generation\n",
    "# and self-consistency are enabled.\n",
    "rates = [0.0, 0.2] #Defines poisoning rates for generation runs\n",
    "\n",
    "for r in rates: #Loops over the selected poisoning rates\n",
    "    cfg_r = replace(cfg_gen, poisoning=replace(cfg_gen.poisoning, enabled=(r > 0.0), rate=r)) #Creates config for this rate\n",
    "    print(\"\\n=== GEN RUN | poison enabled:\", cfg_r.poisoning.enabled, \"rate:\", cfg_r.poisoning.rate, \"===\") #Prints run info\n",
    "\n",
    "    pipeline_fever = RAGTrustPipeline(cfg_r, corpus_fever) #Builds the pipeline for this generation run\n",
    "    res = run_fever(cfg_r, pipeline_fever, fever_ex) #Runs FEVER evaluation with generation enabled\n",
    "    print(\"FEVER summary:\", res[\"summary\"]) #Prints summary results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b9501b91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall@10: 0.9671052631578947\n"
     ]
    }
   ],
   "source": [
    "from ragtrust.evaluation.diagnostics import evidence_recall_at_k #Imports recall@k diagnostic metric\n",
    "\n",
    "N = 200 #Sets number of examples used for diagnostics\n",
    "print(\"Recall@10:\", evidence_recall_at_k(pipeline_fever_base, fever_ex[:N], k=10)) #Computes and prints recall@10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e567d18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conflict %: 0.3881578947368421\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm #Imports progress bar utility\n",
    "\n",
    "conflicts = 0 #Counts how many claims show evidence conflict\n",
    "N = min(len(fever_ex), 200) #Limits analysis to at most 200 examples\n",
    "\n",
    "for ex in tqdm(fever_ex[:N], desc=\"Conflict analysis\", leave=False): #Loops over examples\n",
    "    out = pipeline_fever_base.run_one(ex.claim) #Runs pipeline on one claim\n",
    "    if out[\"verification\"].get(\"conflict\", False): #Checks if verifier detected conflicting evidences\n",
    "        conflicts += 1 #Increments conflict counter\n",
    "\n",
    "print(\"Conflict %:\", conflicts / N) #Prints percentage of conflicting claims"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envtwo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
