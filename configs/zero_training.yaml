# Main configuration file for the RAGTrust project
seed: 42 #Random seed used to make experiments reproducible

data:
  fever:
    enabled: true #Enables the FEVER dataset
    train_jsonl: "Data/Fever/train.jsonl" #Path to FEVER training data
    wiki_pages_dir: "Data/Fever/wiki-pages" #Directory with Wikipedia pages
    max_examples: 200 #Limits the number of FEVER examples used
  hotpotqa:
    enabled: true #Enables the HotpotQA dataset
    train_json: "C:/Users/chiar/hotpot_dev_distractor_v1.jsonl" #Path to HotpotQA data
    max_examples: 200 #Limits the number of HotpotQA examples used

retrieval:
  k: 20 #Number of passages retrieved for each query
  use_bm25: true #Enables sparse BM25 retrieval
  use_dense: true #Enables dense embedding-based retrieval
  dense_model: "sentence-transformers/all-MiniLM-L6-v2" #Model used for dense embeddings
  bm25_tokenizer: "simple" #Tokenizer used by BM25
  max_corpus_sentences: 200000 #Maximum number of sentences indexed from the corpus

poisoning:
  enabled: false #Enables or disables data poisoning
  rate: 0.1 #Fraction of poisoned passages (ignored if disabled)
  strategy: "contradictory_passage" #Type of poisoning strategy
  target: "retrieval_set" #Applies poisoning to retrieved passages only

verification:
  nli_model: "roberta-large-mnli" #NLI model used for verification
  contradiction_threshold: 0.55 #Threshold to consider contradiction strong
  entailment_threshold: 0.55 #Threshold to consider entailment strong
  top_n_verify: 7 #Number of top retrieved passages verified with NLI

generation:
  enabled: false #Enables or disables answer generation
  provider: "hf" #Generation provider (HuggingFace)
  max_new_tokens: 64 #Maximum number of generated tokens
  hf_model: "google/flan-t5-base" #Model used for text generation
  num_samples: 1 #Number of generated answers per query

evaluation:
  compute_self_consistency: true #Enables self-consistency evaluation

training:
  enabled: false #Enables or disables training
  output_dir: "outputs" #Directory where training outputs are saved

  # NLI fine-tuning (FEVER)
  nli:
    base_model: "roberta-large-mnli" #Base NLI model for fine-tuning
    epochs: 1 #Number of training epochs
    batch_size: 8 #Batch size for NLI training
    lr: 2e-5 #Learning rate for NLI training
    max_train_pairs: 2000 #Maximum number of training pairs

  # Retriever fine-tuning (Sentence-Transformers bi-encoder)
  retriever:
    base_model: "sentence-transformers/all-MiniLM-L6-v2" #Base retriever model
    epochs: 1 #Number of training epochs
    batch_size: 32 #Batch size for retriever training
    lr: 2e-5 #Learning rate for retriever training
    max_train_pairs: 2000 #Maximum number of training pairs
